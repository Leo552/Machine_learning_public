{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class DecisionTree:\n",
    "    \n",
    "    def __init__(self, criterion = 'entropy', max_depth = 5): # Instantiate the class\n",
    "        self.name = 'Decision tree model'\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def fit(X,y): # Make the tree\n",
    "        if criterion == 'entropy': # The tree uses the entropy loss at each stage to determine the conditions of each branch\n",
    "            # The decision tree is a greedy algorithm so will pick the best option at each stage i.e. largest entropy loss\n",
    "            get_sublists(X,y, 0)\n",
    "            \n",
    "    def get_sublists(X,y, depth_counter):\n",
    "        best_information_gain = 0 # This will contain the current best total information gain\n",
    "        best_information_gain_index = 0 # Index of the current best\n",
    "\n",
    "        for example in range(len(X)): # Assumes a complete dataset\n",
    "            # Runs through each example choosing it as the pivot point - may repeat\n",
    "\n",
    "            for label in range(len(X[example])): # Runs through each label\n",
    "\n",
    "                # Creates two dictionaries that contain the actual class distribution above and below the data\n",
    "                dict_1 = {0:0,1:0,2:0, 'total':0}\n",
    "                dict_2 = {0:0,1:0,2:0, 'total':0} # The total will make it easier to calculate the probablilities of each class later\n",
    "                for data_point in range(len(X)):\n",
    "                    if X[data_point][label] <= X[example][label]: # Get the distribution of the data either side of the pivot\n",
    "                        dict_1[y[data_point]] += 1\n",
    "                        dict_1['total'] += 1\n",
    "                    else:\n",
    "                        dict_2[y[data_point]] += 1\n",
    "                        dict_1['total'] += 1\n",
    "\n",
    "                total_information_gain = (1 - get_entropy(dict_1)) + (1 - get_entropy(dict_2)) # This value may be greater than 1\n",
    "\n",
    "                if total_information_gain > best_information_gain:\n",
    "                    best_information_gain = total_information_gain\n",
    "                    best_information_gain_index = [label, example]\n",
    "\n",
    "        best_label = best_information_gain_index[0]\n",
    "        best_pivot = best_information_gain_index[1]\n",
    "        \n",
    "        list_1_X, list_1_y, list_2_X, list_2_y = build_sublists(X, best_pivot, best_label)\n",
    "        depth_counter += 1\n",
    "        \n",
    "        if depth_counter < self.max_depth:\n",
    "            list_1_X = get_sublists(list_1_X, list_1_y, depth_counter)\n",
    "            list_2_X = get_sublists(list_2_X, list_2_y, depth_counter)\n",
    "        \n",
    "        return [list_1_X, list_2_X]\n",
    "\n",
    "        ### Build the new lists \n",
    "        ### Make this a recursive function\n",
    "        ### Test it out\n",
    "\n",
    "    def build_sublists(X, pivot, label): # Build two list with the pivot and label provided\n",
    "        list_1_X = []\n",
    "        list_1_y = []\n",
    "        list_2_X = []\n",
    "        list_2_y = []\n",
    "        for example in range(len(X)):\n",
    "            if X[example][label] <= pivot:\n",
    "                list_1_X.append(X[example])\n",
    "                list_1_y.append(y[example])\n",
    "            else:\n",
    "                list_2_X.append(X[example])\n",
    "                list_2_y.append(y[example])\n",
    "        \n",
    "        return list_1_X, list_1_y, list_2_X, list_2_y\n",
    "    \n",
    "    def get_entropy(distribution_of_classes): # recieves a dictionary of the distribution of the classes\n",
    "        entropy = 0\n",
    "        for class_ in range(len(distribution_of_classes) - 1): # Minus 1 to disclude the total part\n",
    "            probability_of_class = distribution_of_classes[class_]/distribution_of_classes['total']\n",
    "            entropy -= probability_of_class * math.log(probability_of_class, 2) # See equation for entropy\n",
    "        \n",
    "        return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
